{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nameless-86/final-nlp/blob/main/tpfinalNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalacion de librerias a usar"
      ],
      "metadata": {
        "id": "HXlWDeFIOXgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "!pip install pymupdf\n",
        "!pip install SPARQLWrapper\n",
        "!pip install tensorflow_text\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "\n",
        "!pip install llama_index sentence-transformers pypdf langchain python-decouple\n",
        "\n"
      ],
      "metadata": {
        "id": "T_vc79uJpoM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librerias"
      ],
      "metadata": {
        "id": "Hn2Pf8gZOaWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from SPARQLWrapper import SPARQLWrapper, XML, JSON\n",
        "import xml.etree.ElementTree as ET\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import chromadb\n",
        "from unidecode import unidecode\n",
        "import fitz\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import os\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from jinja2 import Template\n",
        "import requests\n",
        "import spacy\n",
        "from decouple import config\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "id": "GgAGDMX_lh18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lectura de los archivos PDF"
      ],
      "metadata": {
        "id": "jfaxu5NuOetx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(pdf_path):\n",
        "    texto = str()\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    for page in doc:\n",
        "        texto += page.get_text()\n",
        "\n",
        "    # Close the PDF file\n",
        "    doc.close()\n",
        "\n",
        "    return texto"
      ],
      "metadata": {
        "id": "uV9_wGa-rWFc"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funcion para normalizar el texto"
      ],
      "metadata": {
        "id": "SoqlIHZOOhEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    # Initialize NLTK components\n",
        "    stop_words = set(stopwords.words(\"spanish\"))\n",
        "    stemmer = SnowballStemmer(\"spanish\")\n",
        "\n",
        "    # Remover acentos\n",
        "    text = unidecode(text)\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words and perform stemming\n",
        "    normalized_words = [\n",
        "        stemmer.stem(word.lower())\n",
        "        for word in words\n",
        "        if word.isalpha() and word.lower() not in stop_words\n",
        "    ]\n",
        "\n",
        "    return \" \".join(normalized_words)"
      ],
      "metadata": {
        "id": "btwccvPErWus"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura y normalizacion"
      ],
      "metadata": {
        "id": "r5FWuO0NOjrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "archivos_pdf = [\"100_conceptos_astr.pdf\", \"Astronomia-64.pdf\", \"curso_astronomia.pdf\"]\n",
        "\n",
        "# Process each PDF file\n",
        "for archivo in archivos_pdf:\n",
        "    file_path = (\n",
        "        \"/content/drive/MyDrive/tpnlp/Libros/\" + archivo\n",
        "    )  # Update with the correct path\n",
        "\n",
        "    # Leer\n",
        "    raw_text = read_pdf(file_path)\n",
        "\n",
        "    # Normalizar\n",
        "    normalized_text = normalize_text(raw_text)\n",
        "\n",
        "    print(f\"Texto normalizado de {archivo}:\\n{normalized_text}\")\n"
      ],
      "metadata": {
        "id": "Y6wY9Oe9rYje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creacion de chunks usando langchain"
      ],
      "metadata": {
        "id": "xpa6QFV0Os6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pdfs = dict()\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "for archivo in archivos_pdf:\n",
        "    txt = splitter.split_text(normalized_text)\n",
        "    dict_pdfs[archivo] = txt\n",
        "\n",
        "dict_pdfs\n"
      ],
      "metadata": {
        "id": "5OYioKvyraX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecutra del dataframe usando pandas"
      ],
      "metadata": {
        "id": "NBpB42rlOwRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_observaciones = pd.read_csv('/content/drive/MyDrive/tpnlp/Tabular/Processed_Atels.csv')"
      ],
      "metadata": {
        "id": "CtaA0YRzrvIF"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de modelo de embeddings"
      ],
      "metadata": {
        "id": "jE93iDusOyv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')"
      ],
      "metadata": {
        "id": "iSvE98Y0r0G5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creacion de base de datos vectorial"
      ],
      "metadata": {
        "id": "9F1NCMD-O0qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.Client()\n",
        "collection = client.get_or_create_collection('astronomia-basics')"
      ],
      "metadata": {
        "id": "8aobmi7zv2eJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store(dict, coll):\n",
        "    for key in dict.keys():\n",
        "        embeddings = embed(dict[key]).numpy().tolist()\n",
        "        chunks = dict[key]\n",
        "        ids = [f\"doc_{key}-parte{i}\" for i in range(1, len(chunks) + 1)]\n",
        "\n",
        "        collection.add(documents=chunks, embeddings=embeddings, ids=ids)\n",
        "\n",
        "\n",
        "store(dict_pdfs, collection)"
      ],
      "metadata": {
        "id": "MotCS_vLv5BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creacion de base de datos de grafos (revisar)\n",
        "\n",
        "Se usa un template de wikidata el cual contiene una lista de planetas y el apellido de su descubridor"
      ],
      "metadata": {
        "id": "rr9pVsDtw3Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "\n",
        "\n",
        "sparql.setQuery(\n",
        "    \"\"\"\n",
        "#Who discovered the most planets? (with list)\n",
        "SELECT\n",
        "  ?discoverer ?discovererLabel\n",
        "  (COUNT(DISTINCT ?planet) as ?count)\n",
        "  (GROUP_CONCAT(DISTINCT(?planetLabel); separator=\", \") as ?planets)\n",
        "WHERE\n",
        "{\n",
        "  ?ppart wdt:P279* wd:Q634 .\n",
        "  ?planet wdt:P31 ?ppart .\n",
        "  ?planet wdt:P61 ?discoverer .\n",
        "  SERVICE wikibase:label {\n",
        "    bd:serviceParam wikibase:language \"es\" .\n",
        "    ?discoverer rdfs:label ?discovererLabel .\n",
        "    ?planet rdfs:label ?planetLabel\n",
        "  }\n",
        "}\n",
        "GROUP BY ?discoverer ?discovererLabel\n",
        "ORDER BY DESC(?count)\n",
        "LIMIT 25\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "sparql.setReturnFormat(XML)\n",
        "# Convertir el objeto Document a una cadena\n",
        "\n",
        "results = sparql.query().convert()\n",
        "\n",
        "# Parsear el resultado XML\n",
        "xml_string = results.toxml()\n",
        "\n",
        "root = ET.fromstring(xml_string)\n",
        "\n",
        "# El espacio de nombres (namespace) que usaremos para extraer los datos\n",
        "namespace = \"{http://www.w3.org/2005/sparql-results#}\"\n"
      ],
      "metadata": {
        "id": "z0MB7Qb0gTfT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Planetas y sus descubridores\\n')\n",
        "print('--------------------------------')\n",
        "# Iterar sobre cada resultado y extraer los datos relevantes\n",
        "for result in root.findall(f\".//{namespace}result\"):\n",
        "\n",
        "  discoverer = result.find(f'.//{namespace}binding[@name=\"discovererLabel\"]/{namespace}literal').text\n",
        "  planet = result.find(f'.//{namespace}binding[@name=\"planets\"]/{namespace}literal').text\n",
        "\n",
        "  print(f'(\"{discoverer}\", \"Descubrio \", \"{planet}\")')"
      ],
      "metadata": {
        "id": "usN-b26UjBR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query sobre la GDB"
      ],
      "metadata": {
        "id": "OYjGsHt4qQKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_md\")\n",
        "\n",
        "procesado_GDB = nlp(xml_string)\n",
        "print(procesado_GDB)"
      ],
      "metadata": {
        "id": "3FIp-DS_qPjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuente de conocimiento (terminar)"
      ],
      "metadata": {
        "id": "o7rG_pvDvQYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "RvtsbpefvSW7"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "TTtqKxPsvtY4"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "TL13b1xy7mEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### funcion para generar respuesta"
      ],
      "metadata": {
        "id": "m55XXmfJ8Qwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(prompt: str, max_new_tokens: int = 768) -> None:\n",
        "    try:\n",
        "        # Tu clave API de Hugging Face\n",
        "        api_key = \"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\"\n",
        "        # URL de la API de Hugging Face para la generación de texto\n",
        "        api_url = (\n",
        "            \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "        )\n",
        "        # Cabeceras para la solicitud\n",
        "        headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "        data = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": max_new_tokens,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_k\": 50,\n",
        "                \"top_p\": 0.95,\n",
        "            },\n",
        "        }\n",
        "        # Realizamos la solicitud POST\n",
        "        response = requests.post(api_url, headers=headers, json=data)\n",
        "        # Extraer respuesta\n",
        "        respuesta = response.json()[0][\"generated_text\"][len(prompt) :]\n",
        "        return respuesta\n",
        "    except Exception as e:\n",
        "        print(f\"Hubo un error {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mCuipLHw7oPa"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir la plantilla Jinja"
      ],
      "metadata": {
        "id": "MDdjaFAr69lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zephyr_chat_template(messages, add_generation_prompt=True):\n",
        "    template_str = \"{% for message in messages %}\"\n",
        "    template_str += \"{% if message['role'] == 'user' %}\"\n",
        "    template_str += \"<|user|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'assistant' %}\"\n",
        "    template_str += \"<|assistant|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% elif message['role'] == 'system' %}\"\n",
        "    template_str += \"<|system|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% else %}\"\n",
        "    template_str += \"<|unknown|>{{ message['content'] }}</s>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "    template_str += \"{% endfor %}\"\n",
        "    template_str += \"{% if add_generation_prompt %}\"\n",
        "    template_str += \"<|assistant|>\\n\"\n",
        "    template_str += \"{% endif %}\"\n",
        "\n",
        "\n",
        "    # Crear un objeto de plantilla con la cadena de plantilla\n",
        "    template = Template(template_str)\n",
        "\n",
        "\n",
        "    # Renderizar la plantilla con los mensajes proporcionados\n",
        "    return template.render(\n",
        "        messages=messages, add_generation_prompt=add_generation_prompt\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ZQOlpaC568oM"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esta función prepara el prompt en estilo QA"
      ],
      "metadata": {
        "id": "MNrCZZspKgiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_prompt(query_str: str, context_info):\n",
        "    PROMPT = (\n",
        "        \"La información de contexto es la siguiente:\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"{context_str}\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"Dada la información de contexto anterior, y sin utilizar conocimiento previo, responde la siguiente pregunta en español.\\n\"\n",
        "        \"Pregunta: {query_str}\\n\"\n",
        "        \"Respuesta: \"\n",
        "    )\n",
        "\n",
        "    # Construimos el contexto de la pregunta\n",
        "    context_str = context_info\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Eres un asistente útil que siempre responde con respuestas veraces, útiles y basadas en hechos.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": PROMPT.format(context_str=context_str, query_str=query_str),\n",
        "        },\n",
        "    ]\n",
        "    final_prompt = zephyr_chat_template(messages)\n",
        "    return final_prompt\n"
      ],
      "metadata": {
        "id": "Mzhuk8vi99Wq"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ej uso (terminar)"
      ],
      "metadata": {
        "id": "HFcx3U6__XDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONSULTAS = [\"que es un agujero negro?\", \"Quien descubrio Urano\", \"La observacion de un pulsar es por rayos X?\"]"
      ],
      "metadata": {
        "id": "rGZX6jvS_Ymk"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer(CONSULTAS[1])"
      ],
      "metadata": {
        "id": "bKTI_ImoLMHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_pregunta(queries):\n",
        "  for query in queries:\n",
        "    final_prompt = prepare_prompt(query_str, str(context_info))\n",
        "    print(\"Pregunta: \", query)\n",
        "    print(\"Respuesta: \")\n",
        "    res = generate_answer(final_prompt)\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "k-Qt_Io5Mdxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creacion de chatbot"
      ],
      "metadata": {
        "id": "nev-p4kpPeV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interface():\n",
        "    print(\"Hola, te puedo ayudar con tus preguntas sobre astronomia (Presione \\'x\\' para salir)\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Pregunta: \")\n",
        "\n",
        "        if user_query == \"x\":\n",
        "            print(\"Nos vemos!\")\n",
        "            break\n",
        "\n",
        "        procesar_pregunta([user_query])\n",
        "\n",
        "interface()"
      ],
      "metadata": {
        "id": "L8prdYUOPgiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1OPn5LTZWr7ccvlcMg8V7oACiN8I5fFmf",
      "authorship_tag": "ABX9TyMk7VLiHbRWKtCOZD7XW6SV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}